{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0b459f3",
   "metadata": {},
   "source": [
    "# On-Prem Iceberg Warehouse with Spark\n",
    "\n",
    "This notebook demonstrates how to query Iceberg tables stored in MinIO using Spark connected to Polaris catalog.\n",
    "All services run on-prem with no cloud connectivity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d4f2b3",
   "metadata": {},
   "source": [
    "## 1. Initialize Spark Session\n",
    "\n",
    "The Spark session is automatically initialized with Polaris catalog configuration via PYSPARK_SUBMIT_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e946eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark session created!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\")\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "POLARIS_USERNAME = os.getenv(\"POLARIS_USERNAME\")\n",
    "POLARIS_PASSWORD = os.getenv(\"POLARIS_PASSWORD\")\n",
    "POLARIS_CATALOG_NAME = os.getenv(\"POLARIS_CATALOG_NAME\", 'demo_catalog')\n",
    "\n",
    "packages = \",\".join(\n",
    "    [\n",
    "        \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "        \"com.amazonaws:aws-java-sdk-bundle:1.12.262\",\n",
    "        \"org.apache.iceberg:iceberg-aws-bundle:1.4.3\",\n",
    "        \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.3\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Polaris + Iceberg with MinIO\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Polaris-Iceberg-MinIO\") \\\n",
    "    .config(\"spark.jars.packages\", packages) \\\n",
    "    .config(\"spark.sql.catalog.polaris\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.polaris.catalog-impl\", \"org.apache.iceberg.rest.RESTCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.polaris.uri\", \"http://polaris:8181/api/catalog\") \\\n",
    "    .config(\"spark.sql.catalog.polaris.credential\", f\"{POLARIS_USERNAME}:{POLARIS_PASSWORD}\") \\\n",
    "    .config(\"spark.sql.catalog.polaris.warehouse\", POLARIS_CATALOG_NAME) \\\n",
    "    .config(\"spark.sql.catalog.polaris.scope\", \"PRINCIPAL_ROLE:ALL\") \\\n",
    "    .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✓ Spark session created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc48f45",
   "metadata": {},
   "source": [
    "## 2. Verify Polaris Catalog Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5846a99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all catalogs available\n",
    "spark.sql(\"SHOW CATALOGS\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7dee41",
   "metadata": {},
   "source": [
    "## 3. Create a Namespace (Schema) in Polaris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26c2c249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   namespace|\n",
      "+------------+\n",
      "|     test_db|\n",
      "|my_warehouse|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a namespace for your tables\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS polaris.test_db\")\n",
    "spark.sql(\"SHOW NAMESPACES IN polaris\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5927e472",
   "metadata": {},
   "source": [
    "## 4. Create Sample Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "faa54856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create a sample table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS polaris.my_warehouse.users (\n",
    "        id INT,\n",
    "        name STRING,\n",
    "        email STRING,\n",
    "        created_date DATE\n",
    "    )\n",
    "    USING ICEBERG\n",
    "    PARTITIONED BY (created_date)\n",
    "\"\"\")\n",
    "\n",
    "print(\"Table created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7e43533-0d40-40e3-bb30-5c1f16f3dada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create a sample table\n",
    "spark.sql(\"\"\"\n",
    "    drop table polaris.my_warehouse.users\n",
    "\"\"\")\n",
    "\n",
    "print(\"Table created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a65d09",
   "metadata": {},
   "source": [
    "## 5. Insert Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4bc9b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Insert sample data\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO polaris.my_warehouse.users VALUES\n",
    "    (1, 'Alice', 'alice@example.com', DATE '2025-01-01'),\n",
    "    (2, 'Bob', 'bob@example.com', DATE '2025-01-02'),\n",
    "    (3, 'Charlie', 'charlie@example.com', DATE '2025-01-03'),\n",
    "    (4, 'Diana', 'diana@example.com', DATE '2025-01-04')\n",
    "\"\"\"\n",
    ")\n",
    "print(\"Data inserted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc5908",
   "metadata": {},
   "source": [
    "## 6. Query Using Schema.TableName (Default Catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b83e543e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------------+------------+\n",
      "| id|   name|              email|created_date|\n",
      "+---+-------+-------------------+------------+\n",
      "|  1|  Alice|  alice@example.com|  2025-01-01|\n",
      "|  2|    Bob|    bob@example.com|  2025-01-02|\n",
      "|  3|Charlie|charlie@example.com|  2025-01-03|\n",
      "|  4|  Diana|  diana@example.com|  2025-01-04|\n",
      "+---+-------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Since polaris is set as default catalog, you can query with just schema.tablename\n",
    "spark.sql(\"use polaris\")\n",
    "\n",
    "result = spark.sql(\"SELECT * FROM my_warehouse.users\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57273bf",
   "metadata": {},
   "source": [
    "## 7. Query with Catalog.Schema.TableName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea7297a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------------+------------+\n",
      "| id|   name|              email|created_date|\n",
      "+---+-------+-------------------+------------+\n",
      "|  3|Charlie|charlie@example.com|  2025-01-03|\n",
      "|  4|  Diana|  diana@example.com|  2025-01-04|\n",
      "+---+-------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can also use full 3-part name\n",
    "result = spark.sql(\"SELECT * FROM polaris.my_warehouse.users WHERE id > 2\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740df20a",
   "metadata": {},
   "source": [
    "## 8. Verify MinIO Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e76f15b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-----------+\n",
      "|   namespace|tableName|isTemporary|\n",
      "+------------+---------+-----------+\n",
      "|my_warehouse|    users|      false|\n",
      "|my_warehouse|   orders|      false|\n",
      "+------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List tables in the namespace\n",
    "spark.sql(\"SHOW TABLES IN polaris.my_warehouse\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515c7b85",
   "metadata": {},
   "source": [
    "## 9. Check Table Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57a5c9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|                  id|      int|   NULL|\n",
      "|                name|   string|   NULL|\n",
      "|               email|   string|   NULL|\n",
      "|        created_date|     date|   NULL|\n",
      "|# Partition Infor...|         |       |\n",
      "|          # col_name|data_type|comment|\n",
      "|        created_date|     date|   NULL|\n",
      "+--------------------+---------+-------+\n",
      "\n",
      "\n",
      "--- Table Properties ---\n",
      "+--------------------+--------------------+\n",
      "|                 key|               value|\n",
      "+--------------------+--------------------+\n",
      "|          created-at|2025-12-11T14:22:...|\n",
      "| current-snapshot-id| 9196138478529762456|\n",
      "|              format|     iceberg/parquet|\n",
      "|      format-version|                   2|\n",
      "|write.parquet.com...|                zstd|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View table schema\n",
    "spark.sql(\"DESCRIBE TABLE polaris.my_warehouse.users\").show()\n",
    "\n",
    "# View table properties\n",
    "print(\"\\n--- Table Properties ---\")\n",
    "spark.sql(\"SHOW TBLPROPERTIES polaris.my_warehouse.users\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3e1af0",
   "metadata": {},
   "source": [
    "## 10. Query with DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "345481d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|   name|              email|\n",
      "+-------+-------------------+\n",
      "|    Bob|    bob@example.com|\n",
      "|Charlie|charlie@example.com|\n",
      "|  Diana|  diana@example.com|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can also use DataFrame API\n",
    "df = spark.table(\"my_warehouse.users\")\n",
    "df.filter(df.id > 1).select(\"name\", \"email\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8868b68b",
   "metadata": {},
   "source": [
    "## 11. Create Another Table to Demonstrate Multi-table Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39d0e4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders table created and populated!\n"
     ]
    }
   ],
   "source": [
    "# Create orders table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS polaris.my_warehouse.orders (\n",
    "        order_id INT,\n",
    "        user_id INT,\n",
    "        amount DECIMAL(10,2),\n",
    "        order_date DATE\n",
    "    )\n",
    "    USING ICEBERG\n",
    "    PARTITIONED BY (order_date)\n",
    "\"\"\")\n",
    "\n",
    "# Insert sample data\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO polaris.my_warehouse.orders VALUES\n",
    "    (101, 1, 150.50, DATE '2025-01-10'),\n",
    "    (102, 2, 200.00, DATE '2025-01-11'),\n",
    "    (103, 1, 75.25, DATE '2025-01-12'),\n",
    "    (104, 3, 300.00, DATE '2025-01-13')\n",
    "\"\"\")\n",
    "\n",
    "print(\"Orders table created and populated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef27e53",
   "metadata": {},
   "source": [
    "## 12. Join Across Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e3f15de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+--------+------+\n",
      "|   name|              email|order_id|amount|\n",
      "+-------+-------------------+--------+------+\n",
      "|  Alice|  alice@example.com|     101|150.50|\n",
      "|  Alice|  alice@example.com|     101|150.50|\n",
      "|    Bob|    bob@example.com|     102|200.00|\n",
      "|    Bob|    bob@example.com|     102|200.00|\n",
      "|  Alice|  alice@example.com|     103| 75.25|\n",
      "|  Alice|  alice@example.com|     103| 75.25|\n",
      "|Charlie|charlie@example.com|     104|300.00|\n",
      "|Charlie|charlie@example.com|     104|300.00|\n",
      "+-------+-------------------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join users and orders\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT u.name, u.email, o.order_id, o.amount\n",
    "    FROM my_warehouse.users u\n",
    "    JOIN my_warehouse.orders o ON u.id = o.user_id\n",
    "    ORDER BY o.order_id\n",
    "\"\"\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787cbe30",
   "metadata": {},
   "source": [
    "## 13. Advanced: Time Travel (Iceberg Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "516edddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+---------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+---------+---------+--------------------+--------------------+\n",
      "|2025-12-11 14:22:...|9196138478529762456|     NULL|   append|s3://warehouse/my...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+---------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View table history\n",
    "spark.sql(\"SELECT * FROM polaris.my_warehouse.users.snapshots\").show()\n",
    "\n",
    "# You can query specific snapshots if needed\n",
    "# spark.sql(\"SELECT * FROM polaris.my_warehouse.users VERSION AS OF 9196138478529762456\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e370058",
   "metadata": {},
   "source": [
    "## 14. Configuration Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d1d9b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark SQL Catalog Config:\n",
      "Default Catalog: spark_catalog\n",
      "Polaris URI: http://host.docker.internal:8181/api/catalog\n",
      "S3 Endpoint: http://minio:9000\n",
      "Warehouse Path: demo_catalog\n"
     ]
    }
   ],
   "source": [
    "# Verify your Spark configuration\n",
    "print(\"Spark SQL Catalog Config:\")\n",
    "print(f\"Default Catalog: {spark.conf.get('spark.sql.defaultCatalog')}\")\n",
    "print(f\"Polaris URI: {spark.conf.get('spark.sql.catalog.polaris.uri')}\")\n",
    "print(f\"S3 Endpoint: {spark.conf.get('spark.hadoop.fs.s3a.endpoint')}\")\n",
    "print(f\"Warehouse Path: {spark.conf.get('spark.sql.catalog.polaris.warehouse')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
